{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "241ad0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/icb/alioguz.can/miniconda3/envs/he_env/bin/python\n",
      "supergpu14.scidom.de\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import socket\n",
    "print(socket.gethostname())\n",
    "import os\n",
    "os.chdir(\"/home/icb/alioguz.can/projects/he_pathalogy\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fe076e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "import squidpy as sq\n",
    "import seaborn  as sns\n",
    "import histomicstk as htk\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from utils import find_full_tiles, plot_top_tiles, quick_tissue_mask, macenko_stain_matrix, separate_stains_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "21c85fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scanning 48 DICOM files in: /lustre/groups/ml01/workspace/alioguz.can/hne_cg/filtered_dicoms\n",
      "\n",
      "âœ… No duplicates by prefix found.\n",
      "\n",
      "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
      "ðŸ“¦ Total DICOM files scanned: 48\n",
      "ðŸ“‚ Duplicate prefix groups:   0\n",
      "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
     ]
    }
   ],
   "source": [
    "path = \"/lustre/groups/ml01/workspace/alioguz.can/hne_cg/filtered_dicoms\"\n",
    "\n",
    "# Collect all DICOMs\n",
    "all_files = sorted([p for p in os.listdir(path) if p.endswith(\".dcm\")])\n",
    "print(f\"ðŸ” Scanning {len(all_files)} DICOM files in: {path}\\n\")\n",
    "\n",
    "# Group by prefix (before first underscore)\n",
    "groups = defaultdict(list)\n",
    "for f in all_files:\n",
    "    prefix = f.split(\"_\")[0]\n",
    "    groups[prefix].append(f)\n",
    "\n",
    "# Find duplicate groups\n",
    "duplicates = {k: v for k, v in groups.items() if len(v) > 1}\n",
    "\n",
    "if duplicates:\n",
    "    print(\"âš ï¸ Duplicate slide prefixes found:\\n\")\n",
    "    for prefix, files in duplicates.items():\n",
    "        print(f\"[{prefix}] â€” {len(files)} DICOMs\")\n",
    "        for f in files:\n",
    "            full_path = os.path.join(path, f)\n",
    "            try:\n",
    "                ds = pydicom.dcmread(full_path, stop_before_pixels=True)\n",
    "                rows, cols = ds.get(\"Rows\", \"?\"), ds.get(\"Columns\", \"?\")\n",
    "                frames = ds.get(\"NumberOfFrames\", \"N/A\")\n",
    "                print(f\"   {f} â†’ {rows}Ã—{cols} pixels, {frames} frames\")\n",
    "            except Exception as e:\n",
    "                print(f\"   {f} â†’ [ERROR reading DICOM: {e}]\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âœ… No duplicates by prefix found.\\n\")\n",
    "\n",
    "# Summary\n",
    "print(\"â€”\" * 80)\n",
    "print(f\"ðŸ“¦ Total DICOM files scanned: {len(all_files)}\")\n",
    "print(f\"ðŸ“‚ Duplicate prefix groups:   {len(duplicates)}\")\n",
    "print(\"â€”\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1224f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELTES DUPLICATE FILES OF SHARED PARENT FOLDERS BY KEEPING THE FILE WITH MORE FRAMES\n",
    "\n",
    "# path = \"/lustre/groups/ml01/workspace/alioguz.can/hne_cg/filtered_dicoms\"\n",
    "\n",
    "# # Collect all DICOMs\n",
    "# all_files = sorted([p for p in os.listdir(path) if p.endswith(\".dcm\")])\n",
    "# print(f\"ðŸ” Scanning {len(all_files)} DICOM files in: {path}\\n\")\n",
    "\n",
    "# # Group by prefix (before first underscore)\n",
    "# groups = defaultdict(list)\n",
    "# for f in all_files:\n",
    "#     prefix = f.split(\"_\")[0]\n",
    "#     groups[prefix].append(f)\n",
    "\n",
    "# # Find duplicate groups\n",
    "# duplicates = {k: v for k, v in groups.items() if len(v) > 1}\n",
    "\n",
    "# deleted_count = 0\n",
    "# kept_count = 0\n",
    "\n",
    "# if duplicates:\n",
    "#     print(\"âš ï¸ Duplicate slide prefixes found and processing for cleanup:\\n\")\n",
    "#     for prefix, files in duplicates.items():\n",
    "#         print(f\"[{prefix}] â€” {len(files)} DICOMs\")\n",
    "\n",
    "#         frame_info = []\n",
    "#         for f in files:\n",
    "#             full_path = os.path.join(path, f)\n",
    "#             try:\n",
    "#                 ds = pydicom.dcmread(full_path, stop_before_pixels=True)\n",
    "#                 frames = int(ds.get(\"NumberOfFrames\", 1))\n",
    "#                 frame_info.append((frames, full_path))\n",
    "#                 print(f\"   {f} â†’ {frames} frames\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"   {f} â†’ [ERROR reading DICOM: {e}]\")\n",
    "\n",
    "#         # Sort by frame count (highest last)\n",
    "#         frame_info.sort(key=lambda x: x[0])\n",
    "#         keep = frame_info[-1]  # highest frame count\n",
    "#         to_delete = frame_info[:-1]\n",
    "\n",
    "#         kept_count += 1\n",
    "#         print(f\"   âœ… Keeping: {os.path.basename(keep[1])} ({keep[0]} frames)\")\n",
    "\n",
    "#         # Delete lower-frame duplicates\n",
    "#         for frames, path_to_remove in to_delete:\n",
    "#             try:\n",
    "#                 os.remove(path_to_remove)\n",
    "#                 deleted_count += 1\n",
    "#                 print(f\"   âŒ Deleted: {os.path.basename(path_to_remove)} ({frames} frames)\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"   [ERROR deleting {path_to_remove}: {e}]\")\n",
    "\n",
    "#         print()\n",
    "# else:\n",
    "#     print(\"âœ… No duplicates by prefix found.\\n\")\n",
    "\n",
    "# # Summary\n",
    "# print(\"â€”\" * 80)\n",
    "# print(f\"ðŸ“¦ Total DICOM files scanned:  {len(all_files)}\")\n",
    "# print(f\"ðŸ“‚ Duplicate prefix groups:    {len(duplicates)}\")\n",
    "# print(f\"âœ… Files kept (highest frames): {kept_count}\")\n",
    "# print(f\"âŒ Files deleted (lower frames): {deleted_count}\")\n",
    "# print(\"â€”\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "62798a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Total DICOM files: 50\n",
      "ðŸ”¹ Unique prefixes: 50\n",
      "\n",
      "E20012878#1#3#3\n",
      "E20012878#1#4#3\n",
      "E20012878#1#5#3\n",
      "E21004292#2#1#3\n",
      "E21006287#1#1#3\n",
      "E21006287#3#1#3\n",
      "E21006287#4#1#3\n",
      "E21006287#5#1#3\n",
      "E21006287#6#1#3\n",
      "E21006288#1#1#3\n",
      "E21006288#2#1#3\n",
      "E21006288#3#1#3\n",
      "E21006288#4#1#3\n",
      "E21006288#5#1#3\n",
      "E21006679#1#1#3\n",
      "E21006679#2#1#3\n",
      "E21006679#3#1#3\n",
      "E21009820#1#1#5\n",
      "E21011531#10#1#3\n",
      "E21011531#11#1#3\n",
      "E21011531#2#1#3\n",
      "E21011531#3#1#3\n",
      "E21011531#5#1#3\n",
      "E21011531#6#1#3\n",
      "E21011531#7#1#3\n",
      "E21011531#8#1#3\n",
      "E21011531#9#1#3\n",
      "KO21000706#1#1#3\n",
      "KO21000706#10#1#3\n",
      "KO21000706#11#1#3\n",
      "KO21000706#2#1#3\n",
      "KO21000706#4#1#3\n",
      "KO21000706#6#1#3\n",
      "KO21000706#7#1#3\n",
      "KO21000739#2#1#3\n",
      "KO21001954#2#1#3\n",
      "KO21001972#2#1#3\n",
      "KO21001972#3#1#3\n",
      "KO21001972#4#1#3\n",
      "KO21001972#7#1#3\n",
      "KO21001972#8#1#3\n",
      "KO21001994#1#1#3\n",
      "KO21001994#1#2#3\n",
      "KO21001994#2#1#3\n",
      "KO21001994#2#2#3\n",
      "KO21001996#1#1#3\n",
      "KO21001997#1#1#3\n",
      "KO21001997#3#1#3\n",
      "KO21001997#5#1#3\n",
      "KO21001997#7#1#3\n"
     ]
    }
   ],
   "source": [
    "root_path = Path(\"/lustre/groups/ml01/workspace/alioguz.can/hne_cg/filtered_dicoms\")\n",
    "\n",
    "# Get all .dcm files\n",
    "all_files = sorted([p for p in os.listdir(root_path) if p.endswith(\".dcm\")])\n",
    "\n",
    "# Extract prefixes (before the first underscore)\n",
    "prefixes = sorted({f.split(\"_\")[0] for f in all_files})\n",
    "\n",
    "# Print results\n",
    "print(f\"ðŸ“¦ Total DICOM files: {len(all_files)}\")\n",
    "print(f\"ðŸ”¹ Unique prefixes: {len(prefixes)}\\n\")\n",
    "\n",
    "for p in prefixes:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "dc08084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "dcm_file_paths = sorted(root_path.rglob(\"*.dcm\"))\n",
    "print(len(dcm_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8364c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_dict = {}\n",
    "\n",
    "for dicom_path in tqdm(dcm_file_paths, desc=\"Processing paths\"):\n",
    "    print(f\"\\nProcessing {dicom_path.name} ...\")\n",
    "\n",
    "    ds = pydicom.dcmread(dicom_path)\n",
    "    image = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "    # Handle possible planar configuration\n",
    "    if getattr(ds, \"PlanarConfiguration\", 0) == 1:\n",
    "        image = np.moveaxis(image, 0, -1)  # (3, H, W) â†’ (H, W, 3)\n",
    "    elif image.shape[-1] != 3:\n",
    "        image = np.transpose(image, (1, 2, 0))  # (3, H, W) â†’ (H, W, 3)\n",
    "\n",
    "    # Ensure (N, H, W, 3)\n",
    "    if image.ndim == 3:\n",
    "        image = image[None, ...]\n",
    "\n",
    "    # --- Per-tile normalization ---\n",
    "    p_low, p_high = 1, 99.5\n",
    "    for i in range(image.shape[0]):\n",
    "        tile = image[i]\n",
    "        lo, hi = np.percentile(tile, [p_low, p_high])\n",
    "        tile = np.clip((tile - lo) / (hi - lo + 1e-6), 0, 1)\n",
    "        image[i] = tile * 255.0\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # print(\"normalized shape:\", image.shape, \"dtype:\", image.dtype)\n",
    "\n",
    "    # Patient ID\n",
    "    patient_id = ds.ContainerIdentifier\n",
    "    # print(f\"Patient: {patient_id}\")\n",
    "\n",
    "    # Skip if already processed\n",
    "    if patient_id not in patient_dict:\n",
    "        patient_dict[patient_id] = {}\n",
    "\n",
    "    # --- Find full tiles ---\n",
    "    full_idx, scores = find_full_tiles(image, SCALE_255=False)\n",
    "    # print(f\"{len(full_idx)} files have signal.\")\n",
    "    full_idx = full_idx\n",
    "\n",
    "    # --- Compute Eosin OD for full tiles ---\n",
    "    E_od_list = []\n",
    "    for idx in full_idx:\n",
    "        tile = image[idx]\n",
    "        tmask = quick_tissue_mask(tile)\n",
    "        W = macenko_stain_matrix(tile, tissue_mask=tmask, beta=0.15, alpha=1.0)\n",
    "        H_od, E_od, _ = separate_stains_od(tile, W)\n",
    "        E_od_list.append(E_od)\n",
    "    E_od_array = np.stack(E_od_list)\n",
    "\n",
    "    # --- Segmentation and counting ---\n",
    "    for i, arr in enumerate(E_od_array):\n",
    "        sq_imgCont = sq.im.ImageContainer(arr, layer=\"image\", mask=None, dims=\"channels_last\")\n",
    "        sq.im.process(sq_imgCont, layer=\"image\", method=\"smooth\", sigma=1)\n",
    "        threshold_val = np.percentile(np.array(sq_imgCont[\"image_smooth\"]).flatten(), 99)\n",
    "        sq.im.segment(img=sq_imgCont, layer=\"image_smooth\", method=\"watershed\", thresh=threshold_val, geq=True)\n",
    "\n",
    "        seg_mask = sq_imgCont[\"segmented_watershed\"].values\n",
    "        labels = np.unique(seg_mask)\n",
    "        n_objects = len(labels) - (1 if 0 in labels else 0)\n",
    "\n",
    "        print(f\"Tile {full_idx[i]}: Thresholding at {threshold_val} -> Number of objects identified: {n_objects}\")\n",
    "        patient_dict[patient_id][f\"tile_{full_idx[i]}\"] = n_objects\n",
    "\n",
    "# --- Convert dictionary to DataFrame ---\n",
    "df = pd.DataFrame.from_dict(patient_dict, orient=\"index\").fillna(0).astype(int)\n",
    "df = df.reindex(sorted(df.columns, key=lambda x: int(x.split(\"_\")[1])), axis=1)\n",
    "df[\"maximum\"] = df.apply(\n",
    "    lambda row: f\"{row.idxmax()}={row.max()}\", axis=1\n",
    ")\n",
    "df.to_csv(\"/home/icb/alioguz.can/projects/he_pathalogy/hne_objects_by_patient_and_tile.csv\")\n",
    "print(f\"\\nPatients: {df.shape[0]} | Tiles: {df.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
